dataset:
  train:
    type: LatinDataset
    args:
      list_file: ./datasets/train_list.txt
      num_channels: 1
    img_size:
      max_h: 32
      max_w: 224
    loader:
      batch_size: 128
      shuffle: false  # Need to be false for using distributed sampler(defalut shuffle:true)
      pin_memory: true
      num_workers: 8
  # val:
  #   type: LatinDataset
  #   args:
  #     list_file: ./datasets/train_list.txt
  #     num_channels: 1
  #   img_size:
  #     max_h: 32
  #     max_w: 224
  #   loader:
  #     batch_size: 1
  #     shuffle: false
  #     pin_memory: true
  #     num_workers: 4

arch:
  encoder:
    in_channels: 1
    num_hidden: 256
  decoder:
    num_hidden: 256
    output_size: 39  # 10 + 26 + 2 + 1  2: GO, EOS, 1: Blank
    dropout_rate: 0.5

trainer:
  teach_forcing_prob: 0.5
  clip_grad_norm: 15
  seed: 2
  epochs: 1000
  log_iter: 20
  outputs: 'outputs/'
  restore: false
  tensorboard: true
  use_benchmark: true

optimizer:
  type: Adadelta
  args:
    lr: 1.
    weight_decay: 0.00001
